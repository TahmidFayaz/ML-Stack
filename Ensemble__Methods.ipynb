{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Max Voting\n"
      ],
      "metadata": {
        "id": "xGBMYaN-c_jf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eap5U8roclzC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statistics as st\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# SPLITTING THE DATASET\n",
        "df = pd.read_csv('heart.csv')\n",
        "x = df.drop('target', axis = 1)\n",
        "y = df['target']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# MODELS CREATION\n",
        "model1 = DecisionTreeClassifier()\n",
        "model2 = KNeighborsClassifier()\n",
        "model3= LogisticRegression()\n",
        "\n",
        "model1.fit(x_train,y_train)\n",
        "model2.fit(x_train,y_train)\n",
        "model3.fit(x_train,y_train)\n",
        "\n",
        "# PREDICTION\n",
        "pred1=model1.predict(x_test)\n",
        "pred2=model2.predict(x_test)\n",
        "pred3=model3.predict(x_test)\n",
        "\n",
        "# FINAL_PREDICTION\n",
        "final_pred = np.array([])\n",
        "for i in range(0,len(x_test)):\n",
        "    final_pred = np.append(final_pred, st.mode([pred1[i], pred2[i], pred3[i]]))\n",
        "print(final_pred)\n",
        "\n",
        "\n",
        "# You loop through the length of the test data (len(x_test)), and for each test instance i, you collect the predictions from the three models (pred1[i], pred2[i], pred3[i]).\n",
        "# The statistics.mode function is used to determine the most common prediction (the majority vote) among the three models for that particular instance.\n",
        "# The mode (most frequent value) is then appended to the final_pred array."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averaging\n"
      ],
      "metadata": {
        "id": "AMrZk7BnuNxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tree.DecisionTreeClassifier()\n",
        "model2 = KNeighborsClassifier()\n",
        "model3= LogisticRegression()\n",
        "\n",
        "model1.fit(x_train,y_train)\n",
        "model2.fit(x_train,y_train)\n",
        "model3.fit(x_train,y_train)\n",
        "\n",
        "pred1=model1.predict_proba(x_test)\n",
        "pred2=model2.predict_proba(x_test)\n",
        "pred3=model3.predict_proba(x_test)\n",
        "\n",
        "finalpred=(pred1+pred2+pred3)/3"
      ],
      "metadata": {
        "id": "IPq2qkyfuNik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted Average"
      ],
      "metadata": {
        "id": "Yo2W53-I2odh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tree.DecisionTreeClassifier()\n",
        "model2 = KNeighborsClassifier()\n",
        "model3= LogisticRegression()\n",
        "\n",
        "model1.fit(x_train,y_train)\n",
        "model2.fit(x_train,y_train)\n",
        "model3.fit(x_train,y_train)\n",
        "\n",
        "pred1=model1.predict_proba(x_test)\n",
        "pred2=model2.predict_proba(x_test)\n",
        "pred3=model3.predict_proba(x_test)\n",
        "\n",
        "finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)"
      ],
      "metadata": {
        "id": "JlQhOgPC2oKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging Meta - Estimator\n"
      ],
      "metadata": {
        "id": "jGBGUDRA4a_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn import tree\n",
        "model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)\n",
        "\n",
        "\n",
        "#Regressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)\n"
      ],
      "metadata": {
        "id": "4M4vsUig4eh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ada Boost\n"
      ],
      "metadata": {
        "id": "zFM72HZ5KG_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "model = AdaBoostClassifier(random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)\n",
        "\n",
        "#Regressor\n",
        "\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "model = AdaBoostRegressor(random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "16vxGL87KGnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting (GBM)"
      ],
      "metadata": {
        "id": "SiUD6KlMSrXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model= GradientBoostingRegressor()\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)"
      ],
      "metadata": {
        "id": "ZKGyu7y3MMZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extreme Gradient Boosting (XGBM)\n"
      ],
      "metadata": {
        "id": "hGWxUKzbX3XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier\n",
        "import xgboost as xgb\n",
        "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)\n",
        "#Regressor\n",
        "import xgboost as xgb\n",
        "model=xgb.XGBRegressor()\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test,y_test)\n",
        "\n",
        "\n",
        "# nthread\n",
        "# This is used for parallel processing and the number of cores in the system should be entered.\n",
        "# eta\n",
        "# Makes the model more robust by shrinking the weights on each step."
      ],
      "metadata": {
        "id": "S6D-UES6YswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Light GBM\n"
      ],
      "metadata": {
        "id": "odT1GGyQcx9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification\n",
        "import lightgbm as lgb\n",
        "train_data=lgb.Dataset(x_train,label=y_train)\n",
        "#define parameters\n",
        "params = {'learning_rate':0.001}\n",
        "model= lgb.train(params, train_data, 100)\n",
        "y_pred=model.predict(x_test)\n",
        "for i in range(0,185):\n",
        "  if y_pred[i]>=0.5:\n",
        "      y_pred[i]=1\n",
        "  else:\n",
        "      y_pred[i]=0\n",
        "\n",
        "\n",
        "\n",
        "#Regression\n",
        "import lightgbm as lgb\n",
        "train_data=lgb.Dataset(x_train,label=y_train)\n",
        "params = {'learning_rate':0.001}\n",
        "model= lgb.train(params, train_data, 100)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "rmse=mean_squared_error(y_pred,y_test)**0.5\n",
        "\n",
        "#num_iterations : It specifies the number of boosting iterations to perform.\n",
        "#num_leaves :In case of Light GBM, since splitting takes place leaf-wise rather than depth-wise, num_leaves must be smaller than 2^(max_depth), otherwise, it may lead to overfitting.\n",
        "#bagging_fractions : It specifies the fraction of data to use for each iteration.\n",
        "#max_bin:A smaller value of max_bin can save a lot of time as it buckets the feature values in discrete bins which is computationally inexpensive.\n"
      ],
      "metadata": {
        "id": "uztkcE2PdNlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CatBoost\n"
      ],
      "metadata": {
        "id": "NEJwRhchfRgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost algorithm effectively deals with categorical variables.\n",
        "# Thus, you should not perform one-hot encoding for categorical variables. Just load the files, impute missing values, and youâ€™re good to go.\n",
        "\n",
        "\n",
        "#Classification\n",
        "from CatBoost import CatBoostClassifier\n",
        "model=CatBoostClassifier()\n",
        "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
        "model.fit(x_train,y_train,cat_features=([ 0,1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\n",
        "model.score(x_test,y_test)\n",
        "#Regression\n",
        "from CatBoost import CatBoostRegressor\n",
        "model=CatBoostRegressor()\n",
        "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
        "model.fit(x_train,y_train,cat_features=([ 0,1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\n",
        "model.score(x_test,y_test)\n",
        "\n",
        "# border_count:It is similar to the max_bin parameter.\n",
        "# loss_function:\n",
        "# Defines the metric to be used for training.\n",
        "# eval_metric:\n",
        "# Defines the metric to be used for validation."
      ],
      "metadata": {
        "id": "1Anr05Xsgamy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}